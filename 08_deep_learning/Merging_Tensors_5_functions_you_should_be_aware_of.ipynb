{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Merging Tensors: 5 functions you should be aware of.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeEjfvAKb8Fn"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdK3xG1hky6C"
      },
      "source": [
        "# **Merging Tensors: 5 functions you should be aware of**\n",
        "\n",
        "\"*PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.*\" \n",
        "\n",
        " -from PyTorch Documentation\n",
        "\n",
        "When it comes to machine learning, we have dealt with pandas DataFrame and ways to merge them. We could see those as merging of 2 dimensional data structures. But since tensors can be of  3 dimensions or even more, it is essential to know the ways in which they can be merged. \n",
        "\n",
        "Here we try and look at 5 functions you should know of to understand different ways of merging tensors.\n",
        "\n",
        "\n",
        "**Function 1 - torch.cat**\n",
        "\n",
        "**Function 2 - torch.stack**\n",
        "\n",
        "**Function 3 - torch.unsqueeze**\n",
        "\n",
        "**Function 4 - torch.hstack**\n",
        "\n",
        "**Function 5 - torch.vstack**\n",
        "\n",
        "\n",
        "\n",
        "**N.B:** \n",
        "\n",
        "Before we dive into merging, it's best to remember the following:\n",
        "dimensions are numbered, starting from 0, similar to python indexing, But merging two 3 D tensors can be tricky and\n",
        "\n",
        "Merging along \n",
        "> **dimension 0** is like merging two tensors **channel-wise**  visually\n",
        "\n",
        "> **dimension 1** is like merging two tensors **row-wise** visually\n",
        "\n",
        "> **dimension 2** is like merging two tensors **column-wise** visually\n",
        "\n",
        "\n",
        "\n",
        "Hence a tensor with shape (2,3,4) would look like it has 2 channels, each containing  3 rows and 4 column (tensor t displayed below for reference) and a tensor with shape (4,1,3) looks like it has 4 channels, each containing a 1 row and 3 columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lRZFh6-cBv7",
        "outputId": "641f8335-0e63-4c77-aeb9-08315d83ca8a"
      },
      "source": [
        "t =torch.tensor([[[4, 4, 4, 4],\n",
        "         [7, 7, 7, 7],\n",
        "         [7, 7, 7, 7]],\n",
        "\n",
        "        [[4, 4, 4, 4],\n",
        "         [7, 7, 7, 7],\n",
        "         [7, 7, 7, 7]]])\n",
        "print(t)\n",
        "print(t.size())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[4, 4, 4, 4],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[4, 4, 4, 4],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]]])\n",
            "torch.Size([2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IkUm5XOtj0K"
      },
      "source": [
        "#**Function 1 - torch.cat**\n",
        "\n",
        "*torch.cat(tensors, dim=0, *, out=None) → Tensor*\n",
        "\n",
        "**Concatenates the given sequence of seq tensors in the given dimension.** All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
        "\n",
        "\n",
        "This is one of the most common ways in which two tensors can be merged. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo-as9Qcl97e"
      },
      "source": [
        "**Note:** In order to make the size of tensors apparent, I'll be using ***torch.full*** to create tensors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un1R9kaxzHAz",
        "outputId": "3fc953e0-edc5-4e83-e4cc-b5651fd29570"
      },
      "source": [
        "import torch\n",
        "\n",
        "#Example 1.1-(woking)\n",
        "\n",
        "t1 =torch.full((2,2,2),4)\n",
        "print('t1:\\n',t1)\n",
        "\n",
        "t2 =torch.full((2,2,2),7)\n",
        "print('t2:\\n',t2)\n",
        "\n",
        "t3 =torch.cat((t1,t2))\n",
        "print('t3:\\n',t3)\n",
        "print('t3 size:\\n',t3.size())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1:\n",
            " tensor([[[4, 4],\n",
            "         [4, 4]],\n",
            "\n",
            "        [[4, 4],\n",
            "         [4, 4]]])\n",
            "t2:\n",
            " tensor([[[7, 7],\n",
            "         [7, 7]],\n",
            "\n",
            "        [[7, 7],\n",
            "         [7, 7]]])\n",
            "t3:\n",
            " tensor([[[4, 4],\n",
            "         [4, 4]],\n",
            "\n",
            "        [[4, 4],\n",
            "         [4, 4]],\n",
            "\n",
            "        [[7, 7],\n",
            "         [7, 7]],\n",
            "\n",
            "        [[7, 7],\n",
            "         [7, 7]]])\n",
            "t3 size:\n",
            " torch.Size([4, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSVAsapph8gd"
      },
      "source": [
        "**In simple words, we specify the tensors we want to concatenate and mention the dimension in which we want the concatenation to occur.**\n",
        "\n",
        "If no dimension is specified, it concatenates along dimension 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytJIlDCz1RrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64dc7254-049e-4464-d956-5c4f57854d14"
      },
      "source": [
        "#The above is same as torch.cat((t1,t2), dim=0) \n",
        "t4 =torch.cat((t1,t2), dim=0)\n",
        "print('t4:\\n',t4)\n",
        "print('t4 size:\\n',t4.size())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t4:\n",
            " tensor([[[4, 4],\n",
            "         [4, 4]],\n",
            "\n",
            "        [[4, 4],\n",
            "         [4, 4]],\n",
            "\n",
            "        [[7, 7],\n",
            "         [7, 7]],\n",
            "\n",
            "        [[7, 7],\n",
            "         [7, 7]]])\n",
            "t4 size:\n",
            " torch.Size([4, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTw-_zEk14B4"
      },
      "source": [
        "**While using torch.cat, tensors can have different sizes, the only condition being only that dimension along which it is being merged can have different values.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGPGRP152mIB",
        "outputId": "1d65518a-68e2-42a6-c133-637086b892d1"
      },
      "source": [
        "#Example 1.2 -(woking)\n",
        "t5 =torch.full((2,1,1),6)\n",
        "print('t5:\\n',t5)\n",
        "\n",
        "t6 =torch.full((2,1,3),8)\n",
        "print('t6:\\n',t6)\n",
        "\n",
        "t7 =torch.cat((t5,t6), dim=2) #merging along dimension 2 (third dimension)\n",
        "print('t7:\\n',t7)\n",
        "print('t7 size:\\n',t7.size())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t5:\n",
            " tensor([[[6]],\n",
            "\n",
            "        [[6]]])\n",
            "t6:\n",
            " tensor([[[8, 8, 8]],\n",
            "\n",
            "        [[8, 8, 8]]])\n",
            "t7:\n",
            " tensor([[[6, 8, 8, 8]],\n",
            "\n",
            "        [[6, 8, 8, 8]]])\n",
            "t7 size:\n",
            " torch.Size([2, 1, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-9vKEYH4P6I"
      },
      "source": [
        "**ERROR:** While using torch.cat, error can occur probably in two ways:\n",
        "\n",
        " Case 1: tensors have different sizes in dimensions other than dimension zero, and dimension is not specified while using torch.cat.\n",
        "\n",
        "Case 2: tensors have different sizes in dimensions other than dimension specified while using torch.cat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "2wi6sUwQ5RBE",
        "outputId": "e0e83314-1a81-42e8-9204-5536d77422b3"
      },
      "source": [
        "#Example 1.31 -(breaking)\n",
        "#Case1\n",
        "\n",
        "t5 =torch.full((2,1,1),6)\n",
        "print('t5:\\n',t5)\n",
        "\n",
        "t6 =torch.full((2,1,3),8)\n",
        "print('t6:\\n',t6)\n",
        "\n",
        "t8 =torch.cat((t5,t6)) # dimension not specified\n",
        "print('t8:\\n',t8)\n",
        "print('t8 size:\\n',t8.size())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t5:\n",
            " tensor([[[6]],\n",
            "\n",
            "        [[6]]])\n",
            "t6:\n",
            " tensor([[[8, 8, 8]],\n",
            "\n",
            "        [[8, 8, 8]]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-923913311dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't6:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mt8\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dimension not specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't8:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't8 size:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 1 and 3 in dimension 2 (The offending index is 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyh59hQ-5n77"
      },
      "source": [
        "#Example 1.32 -(breaking)\n",
        "#Case2\n",
        "\n",
        "t5 =torch.full((2,1,1),6)\n",
        "print('t5:\\n',t5)\n",
        "\n",
        "t6 =torch.full((2,1,3),8)\n",
        "print('t6:\\n',t6)\n",
        "\n",
        "t9 =torch.cat((t5,t6), dim=1) # merging along dimension 1 (Second dimension)\n",
        "print('t9:\\n',t9)\n",
        "print('t9 size:\\n',t9.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU1V2mTPpeGq"
      },
      "source": [
        "## **SUMMARY**\n",
        "\n",
        "*torch.cat* **can be used when we need to merge two tensors along a dimension specified by us, as long as the sizes of dimensions** (other than the one along which we are merging) **remains the same.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UonQQUkbhbrp"
      },
      "source": [
        "# **Function 2 - torch.stack**\n",
        "\n",
        "*torch.stack(tensors, dim=0, *, out=None) → Tensor*\n",
        "\n",
        "**Concatenates a sequence of tensors along a new dimension.** All tensors need to be of the same size.\n",
        "\n",
        "\n",
        "To see the difference between **torch.cat** and **torch.stack**, let's take example 1 used for torch.cat and use torch.stack instead. (Remember that in example 1, the dimension was zero)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "segiZxIFjJM5",
        "outputId": "ca48adbd-f37f-45aa-b1c8-6fadc1b03545"
      },
      "source": [
        "#Example 2.1-(woking) (same example used in torch.cat (exmaple 1.1))\n",
        "\n",
        "t1 =torch.full((2,2,2),4)\n",
        "print('t1:\\n',t1)\n",
        "\n",
        "t2 =torch.full((2,2,2),7)\n",
        "print('t2:\\n',t2)\n",
        "\n",
        "t10 =torch.stack((t1,t2))\n",
        "print('t10:\\n',t10)\n",
        "print('t10 size:\\n',t10.size())"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1:\n",
            " tensor([[[4, 4],\n",
            "         [4, 4]],\n",
            "\n",
            "        [[4, 4],\n",
            "         [4, 4]]])\n",
            "t2:\n",
            " tensor([[[7, 7],\n",
            "         [7, 7]],\n",
            "\n",
            "        [[7, 7],\n",
            "         [7, 7]]])\n",
            "t10:\n",
            " tensor([[[[4, 4],\n",
            "          [4, 4]],\n",
            "\n",
            "         [[4, 4],\n",
            "          [4, 4]]],\n",
            "\n",
            "\n",
            "        [[[7, 7],\n",
            "          [7, 7]],\n",
            "\n",
            "         [[7, 7],\n",
            "          [7, 7]]]])\n",
            "t10 size:\n",
            " torch.Size([2, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH8XsNxbjeST"
      },
      "source": [
        "**By looking at the output size, it is clear that the output has 4 dimensions. So what actually happened? In order to explain that, we need to introduce a new function,** *torch.unsqueeze*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQsNf34bkWBD"
      },
      "source": [
        " \n",
        " # **Function 3 - torch.unsqueeze**\n",
        "\n",
        "*torch.unsqueeze(input, dim) → Tensor*\n",
        "\n",
        "**Returns a new tensor with a dimension of size one inserted at the specified position.**The returned tensor shares the same underlying data with this tensor.\n",
        "    \n",
        "\n",
        "\n",
        "A dim value within the range [-input.dim() - 1, input.dim() + 1) can be used. Negative dim will correspond to unsqueeze() applied at dim = dim + input.dim() + 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdJjVDzHk0fX",
        "outputId": "1a755d7e-ca45-4b80-c431-1faf8a114d88"
      },
      "source": [
        "# Example 3.1 - (working)\n",
        "\n",
        "t1 =torch.full((2,2,2),4)\n",
        "print('t1:\\n',t1)\n",
        "\n",
        "t1_unsqueezed =torch.unsqueeze(t1, dim=0) #unsqueezed along dim 0\n",
        "print('t1_unsqueezed:\\n',t1_unsqueezed)\n",
        "print('t1_unsqueezed size:\\n',t1_unsqueezed.size())\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1:\n",
            " tensor([[[4, 4],\n",
            "         [4, 4]],\n",
            "\n",
            "        [[4, 4],\n",
            "         [4, 4]]])\n",
            "t1_unsqueezed:\n",
            " tensor([[[[4, 4],\n",
            "          [4, 4]],\n",
            "\n",
            "         [[4, 4],\n",
            "          [4, 4]]]])\n",
            "t1_unsqueezed size:\n",
            " torch.Size([1, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abw3sJOfl71Z"
      },
      "source": [
        "We can see that what torch.unsqueeze did is that, it **returned a tensor with size 1 introduced at the specified dimension (dimension 0), pushing the original size at dimension 0 to dimension; original size at dimension 1 to dimension 2 and so on, increasing the size of the initial tensor by 1 additional dimension.**\n",
        "\n",
        "The best way to understand this is by considering an analogy. \n",
        "\n",
        "Let's take a 3 dimensional tensor as 3 persons standing in a straight line. \n",
        "\n",
        " \n",
        "When we unsqueeze that tensor along dimension 0, we are introducing a new person of size 1 at the left most position of the line, and now the line has 4 persons, just like a tensor having 4 dimensions.\n",
        "\n",
        "When we unsqueeze that tensor along dimension 1, we are introducing a new person of size 1 at the second position from left, pushing the current 2nd positioned person into 3rd position and current 3rd positioned person into position 4, while the person at position 1 remains there, and now the line has 4 persons, just like a tensor having 4 dimensions.\n",
        "\n",
        "Similar way, unsqueeze can introduce a new dimension of size 1 at dimensions specifed by us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibRagD68nqIq",
        "outputId": "f6b44c5a-ff91-40bb-b9ff-fa1c09a8a920"
      },
      "source": [
        "# Example 3.2 - (working)\n",
        "t6 =torch.full((2,1,3),8)\n",
        "print('t6:\\n',t6)\n",
        "\n",
        "t6_unsqueezed =torch.unsqueeze(t6, dim=-1) \n",
        "print('t6_unsqueezed:\\n',t6_unsqueezed)\n",
        "print('t6_unsqueezed size:\\n',t6_unsqueezed.size())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t6:\n",
            " tensor([[[8, 8, 8]],\n",
            "\n",
            "        [[8, 8, 8]]])\n",
            "t6_unsqueezed:\n",
            " tensor([[[[8],\n",
            "          [8],\n",
            "          [8]]],\n",
            "\n",
            "\n",
            "        [[[8],\n",
            "          [8],\n",
            "          [8]]]])\n",
            "t6_unsqueezed size:\n",
            " torch.Size([2, 1, 3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nst24mSjsR3o"
      },
      "source": [
        "**similar to how -1 in a python list means last entry, here dim= -1 means, tensor is unsqueezed along dim 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1sZ96KIsek2"
      },
      "source": [
        "**ERROR:** \n",
        "\n",
        "Unsqueeze can introduce an additional dimension of size 1 but it has a range of [-input.dim() - 1, input.dim() + 1) only.\n",
        "\n",
        "this means a 3 dimensional tensor has a range of dimensional value [-4,3]. if we specify a dimension beyond this range, it will display an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "rcZAHmoVtVl6",
        "outputId": "f06f4965-754e-45d5-fba0-cecb38ba022b"
      },
      "source": [
        "# Example 3.3 - (breaking)\n",
        "\n",
        "t2 =torch.full((2,2,2),7)\n",
        "print('t2:\\n',t2)\n",
        "\n",
        "t2_unsqueezed =torch.unsqueeze(t2, dim= 4) \n",
        "print('t2_unsqueezed:\\n',t2_unsqueezed)\n",
        "print('t2_unsqueezed size:\\n',t2_unsqueezed.size())"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t2:\n",
            " tensor([[[7, 7],\n",
            "         [7, 7]],\n",
            "\n",
            "        [[7, 7],\n",
            "         [7, 7]]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-c465f6758ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't2:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mt2_unsqueezed\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't2_unsqueezed:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2_unsqueezed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't2_unsqueezed size:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2_unsqueezed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-4, 3], but got 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLsiQZy6t8UB"
      },
      "source": [
        "## **SUMMARY**\n",
        "\n",
        "*torch.unsqueeze* **can be used when we need to create a new tensor with one additional dimension of size 1, at a desired dimension, from an already existing tensor.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho_nfCHczTkk"
      },
      "source": [
        "**NOW LET'S GET BACK TO** torch.stack()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJPjc6fVzdFZ",
        "outputId": "40688c16-09e2-427f-9eb8-ede8cba800f9"
      },
      "source": [
        "#unsqueezing t2 along dimension 0\n",
        "\n",
        "t2 =torch.full((2,2,2),7)\n",
        "print('t2:\\n',t2)\n",
        "\n",
        "t2_unsqueezed =torch.unsqueeze(t2, dim= 0) \n",
        "print('t2_unsqueezed:\\n',t2_unsqueezed)\n",
        "print('t2_unsqueezed size:\\n',t2_unsqueezed.size())\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t2:\n",
            " tensor([[[7, 7],\n",
            "         [7, 7]],\n",
            "\n",
            "        [[7, 7],\n",
            "         [7, 7]]])\n",
            "t2_unsqueezed:\n",
            " tensor([[[[7, 7],\n",
            "          [7, 7]],\n",
            "\n",
            "         [[7, 7],\n",
            "          [7, 7]]]])\n",
            "t2_unsqueezed size:\n",
            " torch.Size([1, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umyeoJqo78hK",
        "outputId": "8adb8ce8-7341-44e3-ff53-8beaadfe8c57"
      },
      "source": [
        "\n",
        "#Revisiting Example 2.1\n",
        "\n",
        "t1 =torch.full((2,2,2),4)\n",
        "print('t1:\\n',t1)\n",
        "\n",
        "t2 =torch.full((2,2,2),7)\n",
        "print('t2:\\n',t2)\n",
        "\n",
        "print('t1_unsqueezed:\\n', t1_unsqueezed)\n",
        "print('t2_unsqueezed:\\n', t2_unsqueezed)\n",
        "\n",
        "#comparing torch.cat and torch.stack\n",
        "\n",
        "t10 =torch.stack((t1,t2))\n",
        "print('t10:\\n',t10)\n",
        "\n",
        "t3_new = torch.cat((t1_unsqueezed,t2_unsqueezed),dim=0)\n",
        "print('t3_new:\\n',t3_new)\n",
        "\n",
        "\n",
        "print('t10 size:\\n',t10.size())\n",
        "print('t3_new size:\\n',t3_new.size())"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1:\n",
            " tensor([[[4, 4],\n",
            "         [4, 4]],\n",
            "\n",
            "        [[4, 4],\n",
            "         [4, 4]]])\n",
            "t2:\n",
            " tensor([[[7, 7],\n",
            "         [7, 7]],\n",
            "\n",
            "        [[7, 7],\n",
            "         [7, 7]]])\n",
            "t1_unsqueezed:\n",
            " tensor([[[[4, 4],\n",
            "          [4, 4]],\n",
            "\n",
            "         [[4, 4],\n",
            "          [4, 4]]]])\n",
            "t2_unsqueezed:\n",
            " tensor([[[[7, 7],\n",
            "          [7, 7]],\n",
            "\n",
            "         [[7, 7],\n",
            "          [7, 7]]]])\n",
            "t10:\n",
            " tensor([[[[4, 4],\n",
            "          [4, 4]],\n",
            "\n",
            "         [[4, 4],\n",
            "          [4, 4]]],\n",
            "\n",
            "\n",
            "        [[[7, 7],\n",
            "          [7, 7]],\n",
            "\n",
            "         [[7, 7],\n",
            "          [7, 7]]]])\n",
            "t3_new:\n",
            " tensor([[[[4, 4],\n",
            "          [4, 4]],\n",
            "\n",
            "         [[4, 4],\n",
            "          [4, 4]]],\n",
            "\n",
            "\n",
            "        [[[7, 7],\n",
            "          [7, 7]],\n",
            "\n",
            "         [[7, 7],\n",
            "          [7, 7]]]])\n",
            "t10 size:\n",
            " torch.Size([2, 2, 2, 2])\n",
            "t3_new size:\n",
            " torch.Size([2, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jktBO8NW-XTt"
      },
      "source": [
        "**From this comparison it is clear that when we torch.stack(t1,t2) along dim =0, what we are essentially doing is unsqueeze t1 and t2 along dim 0, and then torch.cat them along dim 0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPEno4eGZMh0",
        "outputId": "3e4d2538-f519-447e-e9ac-51c0d963ae8a"
      },
      "source": [
        "\n",
        "# Example 2.2 - (working)\n",
        "\n",
        "t11 =torch.full((2,6,7,5),4)\n",
        "#print('t11:\\n',t11)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "\n",
        "t12 =torch.full((2,6,7,5),7)\n",
        "#print('t12:\\n',t12)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "\n",
        "t13 =torch.stack((t11,t12), dim=-1)\n",
        "#print('t13:\\n',t13)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "print('t13 size:\\n',t13.size())"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t13 size:\n",
            " torch.Size([2, 6, 7, 5, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNmU8oqvKqjO"
      },
      "source": [
        "**Just like torch.unsqueeze, dimensions can be in the range of [-input.dim() - 1, input.dim() + 1)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "e1UKW5XT_6qu",
        "outputId": "f8c59342-f47b-4cef-e5b8-cf50bcf7057e"
      },
      "source": [
        "# Example 2.3 - (breaking)\n",
        "\n",
        "t11 =torch.full((2,6,7,5),4)\n",
        "#print('t11:\\n',t11)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "\n",
        "t14 =torch.full((2,6,7,1),7)\n",
        "#print('t14:\\n',t14)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "\n",
        "t15 =torch.stack((t11,t14), dim= 0)\n",
        "#print('t15:\\n',t15)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "print('t15 size:\\n',t15.size())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-9509c3f5b41c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print('t14:\\n',t14)          # commenting this out due to the lengthy result.Uncomment this to get the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mt15\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#print('t15:\\n',t15)          # commenting this out due to the lengthy result.Uncomment this to get the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't15 size:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt15\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 6, 7, 5] at entry 0 and [2, 6, 7, 1] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4seeAUZxLebU"
      },
      "source": [
        "**For torch.stack to work, both tensors should have same size in all dimensions. If the size is not matching even in one dimension, it will display error.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtWTHtneL9sa"
      },
      "source": [
        "## **SUMMARY**\n",
        "\n",
        "*torch.stack* **can be used when we need to concatenate two tensors along a new dimension.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snrO2B1zLznJ"
      },
      "source": [
        " \n",
        " # **Function 4 - torch.hstack**\n",
        "\n",
        "*torch.hstack(tensors, *, out=None) → Tensor*\n",
        "\n",
        "**Stack tensors in sequence horizontally (column wise).** This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.\n",
        "\n",
        "torch.hstack is a **special case of torch.cat** where merging column-wise is the only acceptable merging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEA7GXkWSvCB",
        "outputId": "08f269f8-faa7-4d03-c127-9b3923b6d136"
      },
      "source": [
        "#Example 4.1 -(working)\n",
        "\n",
        "t16 =torch.full((1,2),4)\n",
        "print('t16:\\n',t16)         \n",
        "\n",
        "t17 =torch.full((1,3),7)\n",
        "print('t17:\\n',t17)          \n",
        "\n",
        "t18 =torch.hstack((t16,t17))\n",
        "print('t18:\\n',t18)         \n",
        "print('t18 size:\\n',t18.size())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t16:\n",
            " tensor([[4, 4]])\n",
            "t17:\n",
            " tensor([[7, 7, 7]])\n",
            "t18:\n",
            " tensor([[4, 4, 7, 7, 7]])\n",
            "t18 size:\n",
            " torch.Size([1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGN-xljcW8FJ"
      },
      "source": [
        "**Since the above two tensors are 2D, merging column-wise means merging along dimension 1. Size of tensors can varying only in dimension 1 here, otherwise an error will be displayed.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZtloQCfW7IH",
        "outputId": "4977de48-2bb5-4d86-b7fa-936b68582679"
      },
      "source": [
        "#Example 4.2 -(working)\n",
        "\n",
        "t19 =torch.full((2,1,4),4)\n",
        "print('t19:\\n',t19)         \n",
        "\n",
        "t20 =torch.full((2,2,4),7)\n",
        "print('t20:\\n',t20)          \n",
        "\n",
        "t21 =torch.hstack((t19,t20))\n",
        "print('t21:\\n',t21)         \n",
        "print('t21 size:\\n',t21.size())"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t19:\n",
            " tensor([[[4, 4, 4, 4]],\n",
            "\n",
            "        [[4, 4, 4, 4]]])\n",
            "t20:\n",
            " tensor([[[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]]])\n",
            "t21:\n",
            " tensor([[[4, 4, 4, 4],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[4, 4, 4, 4],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]]])\n",
            "t21 size:\n",
            " torch.Size([2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfGNyHaBc9uG"
      },
      "source": [
        "**Although we know that hstack merges two tensors column-wise, it looks completely different when we look at the input and output tensors.** \n",
        "\n",
        "**If you look carefully, you can see that both input tensors looks like having 2 channels, each containing 1 row and 4 columns for the first tensor (t19) ; while each containing 2 rows and 4 columns for the second tensor(t20).**\n",
        "\n",
        "**Moreover, the output looks as if first channel of t19 combined with first channel of t20 row-wise and the same happened to second channel of t19 and t20.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "rCgi0iNuSvhO",
        "outputId": "da448738-e70c-47a9-9f16-08d6b2731ebf"
      },
      "source": [
        "#Example 4.3 -(breaking)\n",
        "\n",
        "t11 =torch.full((2,6,7,5),4)\n",
        "#print('t11:\\n',t11)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "\n",
        "t14 =torch.full((2,6,7,1),7)\n",
        "#print('t14:\\n',t14)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "\n",
        "t22 =torch.hstack((t11,t14))\n",
        "#print('t22:\\n',t22)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "print('t22 size:\\n',t22.size())"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-e6234f9cccd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print('t14:\\n',t14)          # commenting this out due to the lengthy result.Uncomment this to get the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mt22\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#print('t22:\\n',t22)          # commenting this out due to the lengthy result.Uncomment this to get the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't22 size:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt22\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Got 5 and 1 in dimension 3 (The offending index is 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuN7tt4XWWEH"
      },
      "source": [
        "**Since torch.hstack only accepts merging column-wise for 2d tensors or dimension 1 for 3 or more dimensional tensors, when two tensors don't have same size in any other dimension other than dimension 1, error is displayed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfFhNnP2o-6v"
      },
      "source": [
        "## **SUMMARY**\n",
        "\n",
        "*torch.hstack* **can be used when we need to concatenate two tensors along a dimension 1 by default.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFLfN5XsWJZq"
      },
      "source": [
        " \n",
        " # **Function 5 - torch.vstack**\n",
        "\n",
        "*torch.vstack(tensors, *, out=None) → Tensor*\n",
        "\n",
        "**Stack tensors in sequence vertically (row wise).** This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by torch.atleast_2d().\n",
        "\n",
        "torch.hstack is a **special case of torch.cat** where merging row-wise is the only acceptable merging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIfoH7u-WVDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e2fb15-efa3-4b89-eaea-4a84280241a5"
      },
      "source": [
        "#Example 5.1 -(working)\n",
        "\n",
        "t16 =torch.full((1,2),4)\n",
        "print('t16:\\n',t16)         \n",
        "\n",
        "t17 =torch.full((3,2),7)\n",
        "print('t17:\\n',t17)          \n",
        "\n",
        "t23 =torch.vstack((t16,t17))\n",
        "print('t23:\\n',t23)         \n",
        "print('t23 size:\\n',t23.size())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t16:\n",
            " tensor([[4, 4]])\n",
            "t17:\n",
            " tensor([[7, 7],\n",
            "        [7, 7],\n",
            "        [7, 7]])\n",
            "t23:\n",
            " tensor([[4, 4],\n",
            "        [7, 7],\n",
            "        [7, 7],\n",
            "        [7, 7]])\n",
            "t23 size:\n",
            " torch.Size([4, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtAfCPw0mH3K"
      },
      "source": [
        "**similar to hstack except that it only accepts row-wise merging.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOi8Ab5hmHQV",
        "outputId": "fa7633de-1348-44e8-90fa-9121337c5b9c"
      },
      "source": [
        "#Example 5.2 -(working)\n",
        "\n",
        "t19 =torch.full((2,3,4),4)\n",
        "print('t19:\\n',t19)         \n",
        "\n",
        "t20 =torch.full((3,3,4),7)\n",
        "print('t20:\\n',t20)          \n",
        "\n",
        "t24 =torch.vstack((t19,t20))\n",
        "print('t24:\\n',t24)         \n",
        "print('t24 size:\\n',t24.size())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t19:\n",
            " tensor([[[4, 4, 4, 4],\n",
            "         [4, 4, 4, 4],\n",
            "         [4, 4, 4, 4]],\n",
            "\n",
            "        [[4, 4, 4, 4],\n",
            "         [4, 4, 4, 4],\n",
            "         [4, 4, 4, 4]]])\n",
            "t20:\n",
            " tensor([[[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]]])\n",
            "t24:\n",
            " tensor([[[4, 4, 4, 4],\n",
            "         [4, 4, 4, 4],\n",
            "         [4, 4, 4, 4]],\n",
            "\n",
            "        [[4, 4, 4, 4],\n",
            "         [4, 4, 4, 4],\n",
            "         [4, 4, 4, 4]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]]])\n",
            "t24 size:\n",
            " torch.Size([5, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEsGoLX5mGq4"
      },
      "source": [
        "**In torch.vstack, merging occurs only along dimension 0, hence we could say that torch.vstack is same as default torch.cat.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r44ko-kn957",
        "outputId": "fe6abc4a-acb7-4ea8-d1a6-5141bb1c5da4"
      },
      "source": [
        "#Comparing torch.vstack and default torch.cat (meaning dim=0)\n",
        "\n",
        "t24 =torch.vstack((t19,t20)) #from the earlier example\n",
        "print('t24:\\n',t24)         \n",
        "print('t24 size:\\n',t24.size()) \n",
        "\n",
        "t25 =torch.cat((t19,t20)) \n",
        "print('t25:\\n',t25)         \n",
        "print('t25 size:\\n',t25.size()) "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t24:\n",
            " tensor([[[4, 4, 4, 4],\n",
            "         [4, 4, 4, 4],\n",
            "         [4, 4, 4, 4]],\n",
            "\n",
            "        [[4, 4, 4, 4],\n",
            "         [4, 4, 4, 4],\n",
            "         [4, 4, 4, 4]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]]])\n",
            "t24 size:\n",
            " torch.Size([5, 3, 4])\n",
            "t25:\n",
            " tensor([[[4, 4, 4, 4],\n",
            "         [4, 4, 4, 4],\n",
            "         [4, 4, 4, 4]],\n",
            "\n",
            "        [[4, 4, 4, 4],\n",
            "         [4, 4, 4, 4],\n",
            "         [4, 4, 4, 4]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]],\n",
            "\n",
            "        [[7, 7, 7, 7],\n",
            "         [7, 7, 7, 7],\n",
            "         [7, 7, 7, 7]]])\n",
            "t25 size:\n",
            " torch.Size([5, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDrkON0YohUs"
      },
      "source": [
        "**HENCE PROVED**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "lFEmaCZnokWP",
        "outputId": "724c5399-a4e0-4b8d-c0a5-4f0b8503d5ad"
      },
      "source": [
        "#Example 5.3 -(breaking)\n",
        "\n",
        "t11 =torch.full((2,6,7,5),4)\n",
        "#print('t11:\\n',t11)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "\n",
        "t14 =torch.full((2,6,7,1),7)\n",
        "#print('t14:\\n',t14)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "\n",
        "t26 =torch.vstack((t11,t14))\n",
        "#print('t26:\\n',t26)          # commenting this out due to the lengthy result.Uncomment this to get the result\n",
        "print('t26 size:\\n',t26.size())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-9dc6ca6d748f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print('t14:\\n',t14)          # commenting this out due to the lengthy result.Uncomment this to get the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mt26\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#print('t26:\\n',t26)          # commenting this out due to the lengthy result.Uncomment this to get the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't26 size:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt26\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 5 and 1 in dimension 3 (The offending index is 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzih9eXDpR8v"
      },
      "source": [
        "**Since torch.vstack only accepts merging row-wise for 2d tensors or dimension 0 for 3 or more dimensional tensors, when two tensors don't have same size in any other dimension other than dimension 0, error is displayed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KWvFo65pRt0"
      },
      "source": [
        "## **SUMMARY**\n",
        "\n",
        "*torch.vstack* **can be used when we need to concatenate two tensors along a dimension 0 by default.**"
      ]
    }
  ]
}